{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19b16909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human.pedestrian.adult\n",
      "human.pedestrian.child\n",
      "human.pedestrian.wheelchair\n",
      "human.pedestrian.stroller\n",
      "human.pedestrian.personal_mobility\n",
      "human.pedestrian.police_officer\n",
      "human.pedestrian.construction_worker\n",
      "animal\n",
      "vehicle.car\n",
      "vehicle.motorcycle\n",
      "vehicle.bicycle\n",
      "vehicle.bus.bendy\n",
      "vehicle.bus.rigid\n",
      "vehicle.truck\n",
      "vehicle.construction\n",
      "vehicle.emergency.ambulance\n",
      "vehicle.emergency.police\n",
      "vehicle.trailer\n",
      "movable_object.barrier\n",
      "movable_object.trafficcone\n",
      "movable_object.pushable_pullable\n",
      "movable_object.debris\n",
      "static_object.bicycle_rack\n",
      "Total number of samples\n",
      "404\n"
     ]
    }
   ],
   "source": [
    "# All classes in the dataset\n",
    "cat = []\n",
    "for i in range(len(nusc.category)):\n",
    "    print(nusc.category[i]['name'])\n",
    "    cat.append(nusc.category[i]['name'])\n",
    "\n",
    "# What we want to detect\n",
    "classes = ['human.pedestrian.adult', 'human.pedestrian.child','human.pedestrian.police_officer','human.pedestrian.construction_worker','vehicle.car','vehicle.bicycle']\n",
    "# \n",
    "pedestrians = ['human.pedestrian.adult', 'human.pedestrian.child','human.pedestrian.police_officer','human.pedestrian.construction_worker']\n",
    "\n",
    "print('Total number of samples')\n",
    "print(len(nusc.sample))\n",
    "total_no_of_samples = len(nusc.sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a358d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/asvath/mobile_robotics/blob/master/nuscenes%20extract%20and%20write%20out%202d%20full%20annotation%20boxes.ipynb\n",
    "\n",
    "from pyquaternion import Quaternion\n",
    "from nuscenes.utils.data_classes import Box\n",
    "from nuscenes.utils.geometry_utils import box_in_image, BoxVisibility\n",
    "import numpy as np\n",
    "\n",
    "def get_sample_data(nusc_object, sample_data_token, box_vis_level=BoxVisibility.ANY, selected_anntokens=None):\n",
    "    \"\"\"\n",
    "    Returns the data path as well as all annotations related to that sample_data(single image).\n",
    "    Note that the boxes are transformed into the current sensor's coordinate frame.\n",
    "    :param sample_data_token: <str>. Sample_data token(image token).\n",
    "    :param box_vis_level: <BoxVisibility>. If sample_data is an image, this sets required visibility for boxes.\n",
    "    :param selected_anntokens: [<str>]. If provided only return the selected annotation.\n",
    "    :return: (data_path <str>, boxes [<Box>], camera_intrinsic <np.array: 3, 3>)\n",
    "    \"\"\"\n",
    "\n",
    "    # Retrieve sensor & pose records\n",
    "    sd_record = nusc_object.get('sample_data', sample_data_token)\n",
    "    cs_record = nusc_object.get('calibrated_sensor', sd_record['calibrated_sensor_token'])\n",
    "    sensor_record = nusc_object.get('sensor', cs_record['sensor_token'])\n",
    "    pose_record = nusc_object.get('ego_pose', sd_record['ego_pose_token'])\n",
    "\n",
    "    sample_record = nusc_object.get('sample',sd_record['sample_token'])\n",
    "    data_path = nusc_object.get_sample_data_path(sample_data_token)\n",
    "\n",
    "    if sensor_record['modality'] == 'camera':\n",
    "        cam_intrinsic = np.array(cs_record['camera_intrinsic'])\n",
    "        imsize = (sd_record['width'], sd_record['height'])\n",
    "    else:\n",
    "        cam_intrinsic = None\n",
    "        imsize = None\n",
    "\n",
    "    # Retrieve all sample annotations and map to sensor coordinate system.\n",
    "    if selected_anntokens is not None:\n",
    "        boxes = list(map(nusc_object.get_box, selected_anntokens))\n",
    "    else:\n",
    "        boxes = nusc_object.get_boxes(sample_data_token)\n",
    "        selected_anntokens = sample_record['anns']\n",
    "\n",
    "    # Make list of Box objects including coord system transforms.\n",
    "    box_list = []\n",
    "    ann_list = []\n",
    "    for box,ann in zip(boxes,selected_anntokens):\n",
    "\n",
    "        # Move box to ego vehicle coord system\n",
    "        box.translate(-np.array(pose_record['translation']))\n",
    "        box.rotate(Quaternion(pose_record['rotation']).inverse)\n",
    "\n",
    "        #  Move box to sensor coord system\n",
    "        box.translate(-np.array(cs_record['translation']))\n",
    "        box.rotate(Quaternion(cs_record['rotation']).inverse)\n",
    "\n",
    "        if sensor_record['modality'] == 'camera' and not \\\n",
    "                box_in_image(box, cam_intrinsic, imsize, vis_level=box_vis_level):\n",
    "            continue\n",
    "\n",
    "        box_list.append(box)\n",
    "        ann_list.append(ann)\n",
    "    #this is for a single sample image\n",
    "    return data_path, box_list, ann_list, cam_intrinsic #single image info\n",
    "\n",
    "def threeD_2_twoD(boxsy,intrinsic): #input is a single annotation box\n",
    "    '''\n",
    "    given annotation boxes and intrinsic camera matrix\n",
    "    outputs the 2d bounding box coordinates as a list (all annotations for a particular sample image)\n",
    "    '''\n",
    "    corners = boxsy.corners()\n",
    "    x = corners[0,:]\n",
    "    y = corners[1,:]\n",
    "    z = corners[2,:]\n",
    "    x_y_z = np.array((x,y,z))\n",
    "    orthographic = np.dot(intrinsic,x_y_z)\n",
    "    perspective_x = orthographic[0]/orthographic[2]\n",
    "    perspective_y = orthographic[1]/orthographic[2]\n",
    "    perspective_z = orthographic[2]/orthographic[2]\n",
    "    \n",
    "    min_x = np.min(perspective_x)\n",
    "    max_x = np.max(perspective_x)\n",
    "    min_y = np.min(perspective_y)\n",
    "    max_y = np.max(perspective_y)\n",
    "    \n",
    "\n",
    "    \n",
    "    return min_x,max_x,min_y,max_y\n",
    "\n",
    "\n",
    "\n",
    "def all_3d_to_2d(boxes,anns,intrinsic): #input 3d boxes, annotation key lists, intrinsic matrix (one image)\n",
    "    x_min=[]\n",
    "    x_max=[]\n",
    "    y_min=[]\n",
    "    y_max =[]\n",
    "    width=[]\n",
    "    height=[]\n",
    "    objects_detected =[]\n",
    "    orig_objects_detected =[]\n",
    "    \n",
    "   \n",
    "    for j in range(len(boxes)): #iterate through boxes\n",
    "        box=boxes[j]\n",
    "        \n",
    "        if box.name in classes: #if the box.name is in the classes we want to detect\n",
    "        \n",
    "            if box.name in pedestrians: \n",
    "                orig_objects_detected.append(\"pedestrian\")\n",
    "            elif box.name == \"vehicle.car\":\n",
    "                orig_objects_detected.append(\"car\")\n",
    "            else:\n",
    "                orig_objects_detected.append(\"cyclist\")\n",
    "            #print(box)\n",
    "            \n",
    "            visibility = nusc.get('sample_annotation', '%s' %anns[j])['visibility_token'] #give annotation key\n",
    "            visibility = int(visibility)\n",
    "\n",
    "            \n",
    "            if visibility > 1: #more than 40% visible in the panoramic view of the the cameras\n",
    "\n",
    "                    \n",
    "                center = box.center #get boxe's center\n",
    "\n",
    "                center = np.dot(intrinsic,center)\n",
    "                center_point = center/(center[2]) #convert center point into image plane\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                if center_point[0] <-100 or center_point[0] > 1700 or center_point[1] <-100 or center_point[1] >1000:\n",
    "                    #if center of bounding box is outside of the image, do not annotate\n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "                    min_x, max_x, min_y, max_y = threeD_2_twoD(box,intrinsic) #converts box into image plane\n",
    "                    w = max_x - min_x\n",
    "                    h = max_y - min_y\n",
    "        \n",
    "        \n",
    "                    x_min.append(min_x)\n",
    "                    x_max.append(max_x)\n",
    "                    y_min.append(min_y)\n",
    "                    y_max.append(max_y)\n",
    "                    width.append(w)\n",
    "                    height.append(h)\n",
    "                    if box.name in pedestrians: \n",
    "                        objects_detected.append(\"pedestrian\")\n",
    "                    elif box.name == \"vehicle.car\":\n",
    "                        objects_detected.append(\"car\")\n",
    "                    else:\n",
    "                        objects_detected.append(\"cyclist\")\n",
    "                    \n",
    "\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "    return x_min,x_max,y_min,y_max,width,height,objects_detected,orig_objects_detected #for a single image\n",
    "\n",
    "def extract_bounding_box(i,camera_name): #give a single sample number and camera name\n",
    "    \n",
    "    '''\n",
    "    input sample number i, camera name\n",
    "    outputs min x, max x, min y max y, width and height of bounding box in image coordinates\n",
    "    2d bounding box\n",
    "    options for camera name : CAM_FRONT, CAM_FRONT_RIGHT, CAM_FRONT_LEFT, CAM_BACK, CAM_BACK_RIGHT,CAM_BACK_LEFT\n",
    "    '''\n",
    "    \n",
    "    nusc.sample[i] #one image\n",
    "    \n",
    "    camera_token = nusc.sample[i]['data']['%s' %camera_name] #one camera, get the camera token \n",
    "\n",
    "    path, boxes, anns, intrinsic_matrix = get_sample_data(nusc,'%s' %camera_token) #gets data for one image\n",
    "    \n",
    "    x_min, x_max,y_min,y_max,width,height, objects_detected,orig_objects_detected = all_3d_to_2d(boxes,anns, intrinsic_matrix)\n",
    "    \n",
    "    return x_min, x_max, y_min, y_max, width, height, path, boxes,intrinsic_matrix, objects_detected,orig_objects_detected\n",
    "    #info for a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f96c916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary that maps class names to IDs\n",
    "class_name_to_id_mapping = {\"car\": 0, \"pedestrian\": 1, \"cyclist\": 2}\n",
    "\n",
    "# Convert the info dict to the required yolo format and write it to disk\n",
    "def convert_to_yolov5(info_dict):\n",
    "    print_buffer = []\n",
    "    \n",
    "    # For each bounding box\n",
    "    for b in info_dict[\"bboxes\"]:\n",
    "        try:\n",
    "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
    "        except KeyError:\n",
    "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
    "        \n",
    "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
    "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2 \n",
    "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
    "        b_width    = (b[\"xmax\"] - b[\"xmin\"])\n",
    "        b_height   = (b[\"ymax\"] - b[\"ymin\"])\n",
    "        \n",
    "        # Normalise the co-ordinates by the dimensions of the image\n",
    "        image_w, image_h, image_c = info_dict[\"image_size\"]  \n",
    "        b_center_x /= image_w \n",
    "        b_center_y /= image_h \n",
    "        b_width    /= image_w \n",
    "        b_height   /= image_h \n",
    "        \n",
    "        #Write the bbox details to the file\n",
    "        print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\n",
    "        \n",
    "    # Name of the file which we have to save\n",
    "    save_file_name = os.path.join(\"data/class\", info_dict[\"filename\"].replace(\"jpg\", \"txt\"))\n",
    "    \n",
    "    # Save the annotation to disk\n",
    "    print(\"\\n\".join(print_buffer), file= open(save_file_name, \"w\"))\n",
    "\n",
    "# Source: https://blog.paperspace.com/train-yolov5-custom-data/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
